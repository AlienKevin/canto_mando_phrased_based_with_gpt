{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 58876 word pairs from can2man_table\n",
      "Loaded 46456 entries from wordshk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 46456/46456 [00:00<00:00, 76252.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 14272 new definitions from wordshk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "can2man_table = defaultdict(list)\n",
    "\n",
    "with open(\"can2man_phrase_table.txt\", \"r\") as input_file:\n",
    "    for line in input_file.read().splitlines():\n",
    "        [can, man] = line.split(\"|\")\n",
    "        can2man_table[can].append(man)\n",
    "    print(f\"Loaded {len(can2man_table)} word pairs from can2man_table\")\n",
    "\n",
    "common_man_words = set()\n",
    "with open(\"common_man_words.txt\", \"r\") as input_file:\n",
    "    for word in input_file.read().splitlines():\n",
    "        common_man_words.add(word)\n",
    "\n",
    "wordshk_table = defaultdict(list)\n",
    "\n",
    "import re\n",
    "\n",
    "def is_ascii(s):\n",
    "    return all(ord(c) < 128 for c in s)\n",
    "\n",
    "def filter_empty_string(strs: list[str]) -> list[str]:\n",
    "    return [s for s in strs if len(s) > 0]\n",
    "\n",
    "def process_eng_definition(d: str) -> list[str]:\n",
    "    no_paren = re.sub(\"([\\(\\[（]).*?([\\)\\]）])\", \"\", d).strip()\n",
    "    no_newline = re.sub(r\"\\n.*\", \"\", no_paren)\n",
    "    return filter_empty_string(\n",
    "        [re.sub(r\"\\b(.+)/(.+)\\b\", \"\\g<1>\", d.strip()) for d in re.split(\"[;|,]\", no_newline) if\\\n",
    "        (not d.strip().lower().startswith(\"literally\")\n",
    "        and not d.strip().lower().startswith(\"usually\")\n",
    "        and not d.strip().lower().startswith(\"this word\")\n",
    "        and not d.strip().lower().startswith(\"note that\")\n",
    "        and not d.strip().lower().startswith(\"for example\")\n",
    "        and not re.search(r\"\\beg\\b\", d)\n",
    "        and not re.search(r\"\\be[.]g[.]\", d)\n",
    "        and not re.search(r\"\\betc\\b\", d)\n",
    "        and not re.search(r\"\\bhomophone\\b\", d)\n",
    "        and not re.search(r\"\\bi[.]e[.]\", d)\n",
    "        and not \"\\\"\" in d\n",
    "        and not (d.strip().lower() == \"surname\" or d.strip().lower() == \"a surname\"))\n",
    "        and is_ascii(d)\n",
    "        and d.count(\" \") <= 5])\n",
    "\n",
    "with open(\"wordshk_can_eng.json\", \"r\") as input_file:\n",
    "    data = json.load(input_file)\n",
    "    print(f\"Loaded {len(data.keys())} entries from wordshk\")\n",
    "    for word, eng in tqdm(data.items()):\n",
    "        variants = word.split(\"/\")\n",
    "        for variant in variants:\n",
    "            # Filter out words like \"⋯嚟⋯去\"\n",
    "            if \"⋯\" in variant:\n",
    "                if variant.count(\"⋯\") > 1:\n",
    "                    continue\n",
    "                dot_index = variant.index(\"⋯\")\n",
    "                if dot_index == 0:\n",
    "                    variant = variant[1:]\n",
    "                elif dot_index == len(variant) - 1:\n",
    "                    variant = variant[:-1]\n",
    "                else:\n",
    "                    # print(f\"skipping {variant}\")\n",
    "                    continue\n",
    "            if not variant in can2man_table and not variant in common_man_words:\n",
    "                defs = [process_eng_definition(d) for d in eng if len(process_eng_definition(d)) >= 1]\n",
    "                if len(defs) >= 1:\n",
    "                    wordshk_table[variant].extend(defs)\n",
    "\n",
    "with open(\"wordshk_phrase_table_eng.json\", \"w+\") as output_file:\n",
    "    json.dump(wordshk_table, output_file, ensure_ascii=False)\n",
    "\n",
    "print(f\"Found {len(wordshk_table.keys())} new definitions from wordshk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: GOOGLE_APPLICATION_CREDENTIALS=translate-english-to-mandarin-c793a8415d64.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling Google Translate API...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 147/147 [00:25<00:00,  5.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google Translate API Finished translation...\n",
      "Translated 14272 English definitions to Mandarin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%env GOOGLE_APPLICATION_CREDENTIALS=translate-english-to-mandarin-c793a8415d64.json\n",
    "\n",
    "from google.cloud import translate_v2 as translate\n",
    "translate_client = translate.Client()\n",
    "\n",
    "wordshk_id_table = defaultdict(list)\n",
    "eng_phrase_to_id = {}\n",
    "max_id = 0\n",
    "\n",
    "for word, defs in wordshk_table.items():\n",
    "    for ds in defs:\n",
    "        ds_id = []\n",
    "        for d in ds:\n",
    "            if not d in eng_phrase_to_id:\n",
    "                eng_phrase_to_id[d] = max_id\n",
    "                max_id += 1\n",
    "            ds_id.append(eng_phrase_to_id[d])\n",
    "        wordshk_id_table[word].append(ds_id)\n",
    "\n",
    "# https://stackoverflow.com/a/312464/6798201\n",
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "def translate_texts(source, target, texts):\n",
    "    # Text can also be a sequence of strings, in which case this method\n",
    "    # will return a sequence of results for each text.\n",
    "    final_results = []\n",
    "    for segment in tqdm(list(chunks(texts, 128))):\n",
    "        results = translate_client.translate(segment, source_language=source, target_language=target)\n",
    "        final_results.extend([result[\"translatedText\"] for result in results])\n",
    "    return final_results\n",
    "\n",
    "assert translate_texts(\"en\", \"zh-hk\", [\"section\", \"length\", \"festival\", \"holiday\"]) == [\"部分\", \"長度\", \"節日\", \"假期\"]\n",
    "\n",
    "print(\"Calling Google Translate API...\")\n",
    "man_phrases = translate_texts(\"en\", \"zh-hk\", list(eng_phrase_to_id.keys()))\n",
    "print(\"Google Translate API Finished translation...\")\n",
    "\n",
    "id_to_man_phrase = dict(zip(eng_phrase_to_id.values(), man_phrases))\n",
    "wordshk_man_table = {word: [[id_to_man_phrase[d_id] for d_id in ds_id] for ds_id in defs_id] for word, defs_id in wordshk_id_table.items()}\n",
    "\n",
    "with open(\"wordshk_phrase_table.json\", \"w+\") as output_file:\n",
    "    json.dump(wordshk_man_table, output_file, ensure_ascii=False)\n",
    "\n",
    "print(f\"Translated {len(wordshk_man_table.keys())} English definitions to Mandarin\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

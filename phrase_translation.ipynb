{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Cantonese to Mandarin phrase table of size 11753\n",
      "[('少少', ['一丁點兒', '一點兒', '一點', '一點點兒', '很少份量', '很少']), ('一上一落', ['一上一下']), ('下', ['一下']), ('一搊', ['一串']), ('啲', ['一些', '些', '某些', '這些']), ('單嘢', ['一件事']), ('件', ['一件']), ('一班', ['一伙', '全班', '那班']), ('單拖', ['一個人']), ('獨贏', ['一個人得頭彩'])]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "can2man_table = defaultdict(list)\n",
    "\n",
    "with open(\"phrase_table.txt\", \"r\") as input_file:\n",
    "    for line in input_file.read().splitlines():\n",
    "        [man_word, can_word] = line.split(\"|\")\n",
    "        can2man_table[can_word].append(man_word)\n",
    "\n",
    "print(f\"Generated Cantonese to Mandarin phrase table of size {len(can2man_table)}\")\n",
    "print(list(can2man_table.items())[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A sample of common traditional characters:  ['蠍', '元', '癰', '額', '攀', '法', '爐', '戊', '莆', '穢']\n"
     ]
    }
   ],
   "source": [
    "common_trad_chars = None\n",
    "\n",
    "with open(\"common_trad_chars.txt\", \"r\") as input_file:\n",
    "    common_trad_chars = set(input_file.read())\n",
    "\n",
    "print(\"A sample of common traditional characters: \", list(common_trad_chars)[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 526508 Mandarin words\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from StarCC import PresetConversion\n",
    "convert = PresetConversion(src='cn', dst='hk', with_phrase=False)\n",
    "\n",
    "# df = pd.read_csv(\"common_man_words.csv\", sep=\"\\t\")\n",
    "# common_man_words = { convert(word) for word in df[\"word\"] }\n",
    "common_man_words = set()\n",
    "with open(\"common_man_words.dict.yaml\", \"r\") as input_file:\n",
    "    for line in input_file.read().splitlines():\n",
    "        if not line.startswith(\"#\"):\n",
    "            word = line.split(\"\\t\")[0]\n",
    "            common_man_words.add(convert(word))\n",
    "\n",
    "with open(\"common_man_words.txt\", \"w+\") as output_file:\n",
    "    for word in common_man_words:\n",
    "        output_file.write(word + \"\\n\")\n",
    "\n",
    "print(f\"Got {len(common_man_words)} Mandarin words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 92568 Cantonese words\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"common_can_words.csv\", sep=\",\")\n",
    "common_can_words = set(df[\"char\"])\n",
    "\n",
    "print(f\"Got {len(common_can_words)} Cantonese words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 48844 common words\n",
      "Added 13684 shared words to can2man_table\n"
     ]
    }
   ],
   "source": [
    "common_words = common_can_words.intersection(common_man_words)\n",
    "\n",
    "num_added_words = 0\n",
    "for word in common_words:\n",
    "    if not word in can2man_table or not word in can2man_table[word]:\n",
    "        num_added_words += 1\n",
    "        can2man_table[word].append(word)\n",
    "\n",
    "with open(\"can2man_phrase_table.txt\", \"w+\") as output_file:\n",
    "    for can, mans in can2man_table.items():\n",
    "        for man in mans:\n",
    "            output_file.write(can + \"|\" + man + \"\\n\")\n",
    "\n",
    "print(f\"Got {len(common_words)} common words\")\n",
    "print(f\"Added {num_added_words} shared words to can2man_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def longest_match_translate(s, phrase_table):\n",
    "    man_phrases: list[list[str]] = []\n",
    "    oov_word = \"\"\n",
    "    while s:\n",
    "        longest_match = None\n",
    "        for phrase in phrase_table:\n",
    "            if s.startswith(phrase) and (longest_match is None or len(phrase) > len(longest_match)):\n",
    "                longest_match = phrase\n",
    "        if longest_match:\n",
    "            if len(oov_word) > 0:\n",
    "                man_phrases.append([oov_word])\n",
    "                oov_word = \"\"\n",
    "            can_original = [longest_match] if len(longest_match) <= 1 and all(c in common_trad_chars for c in longest_match) else []\n",
    "            man_phrase = phrase_table[longest_match]\n",
    "            man_phrases.append(can_original + man_phrase)\n",
    "            s = s[len(longest_match):].lstrip()\n",
    "        else:\n",
    "            oov_word += s[0]\n",
    "            s = s[1:].lstrip()\n",
    "    if len(oov_word) > 0:\n",
    "        man_phrases.append([oov_word])\n",
    "    # Merge anchor phrases (those with a single mandarin translation)\n",
    "    i = 0\n",
    "    merged_man_phrases = []\n",
    "    while i < len(man_phrases):\n",
    "        merged_phrase = \"\"\n",
    "        while i < len(man_phrases) and len(man_phrases[i]) == 1:\n",
    "            merged_phrase += man_phrases[i][0]\n",
    "            i += 1\n",
    "        if len(merged_phrase) > 0:\n",
    "            merged_man_phrases.append([merged_phrase])\n",
    "            merged_phrase = \"\"\n",
    "        else:\n",
    "            merged_man_phrases.append(man_phrases[i])\n",
    "            i += 1\n",
    "    return merged_man_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['不好意思', '勞', '勞駕', '有勞您', '請你', '請', '請教', '謝', '謝謝', '麻煩您'],\n",
       " ['你小聲點，我'],\n",
       " ['在此', '在這裡', '在這邊', '在那裡', '在那邊'],\n",
       " ['正在做'],\n",
       " ['事情', '東西'],\n",
       " ['。']]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "longest_match_translate(\"唔該你細聲啲，我喺度做緊嘢。\", can2man_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast, GPT2LMHeadModel\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese')\n",
    "model = GPT2LMHeadModel.from_pretrained('ckiplab/gpt2-base-chinese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import torch\n",
    "\n",
    "def score_sentence_ppl(s: str) -> float:\n",
    "    test = Dataset.from_dict({\n",
    "        \"text\": [s],\n",
    "    })\n",
    "    encodings = tokenizer(\"\\n\\n\".join(test[\"text\"]), return_tensors=\"pt\")\n",
    "\n",
    "    max_length = model.config.n_positions\n",
    "    stride = 512\n",
    "    seq_len = encodings.input_ids.size(1)\n",
    "\n",
    "    nlls = []\n",
    "    prev_end_loc = 0\n",
    "    for begin_loc in range(0, seq_len, stride):\n",
    "        end_loc = min(begin_loc + max_length, seq_len)\n",
    "        trg_len = end_loc - prev_end_loc  # may be different from stride on last loop\n",
    "        input_ids = encodings.input_ids[:, begin_loc:end_loc]\n",
    "        target_ids = input_ids.clone()\n",
    "        target_ids[:, :-trg_len] = -100\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, labels=target_ids)\n",
    "\n",
    "            # loss is calculated using CrossEntropyLoss which averages over input tokens.\n",
    "            # Multiply it with trg_len to get the summation instead of average.\n",
    "            # We will take average over all the tokens to get the true average\n",
    "            # in the last step of this example.\n",
    "            neg_log_likelihood = outputs.loss * trg_len\n",
    "\n",
    "        nlls.append(neg_log_likelihood)\n",
    "\n",
    "        prev_end_loc = end_loc\n",
    "        if end_loc == seq_len:\n",
    "            break\n",
    "\n",
    "    ppl = torch.exp(torch.stack(nlls).sum() / end_loc).item()\n",
    "    return ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex\n",
    "\n",
    "# match any unicode punctuation character and anything after it\n",
    "punctuation_pattern = regex.compile(r\"\\p{P}+.*\", flags=regex.UNICODE)\n",
    "chinese_char_pattern = regex.compile(r\"[\\u4e00-\\u9fff]\")\n",
    "\n",
    "def chop_off_at_punctuation(s: str) -> str:\n",
    "    match = punctuation_pattern.search(s)\n",
    "    if match:\n",
    "        index = match.start()\n",
    "        return s[:index]\n",
    "    else:\n",
    "        return s\n",
    "\n",
    "def chop_off_at_canto_char(s: str) -> str:\n",
    "    for i, c in enumerate(s):\n",
    "        if chinese_char_pattern.match(c) and not c in common_trad_chars:\n",
    "            return s[:i]\n",
    "    return s\n",
    "\n",
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]\n",
    "\n",
    "def can2man(s: str) -> str:\n",
    "    man_phrases = longest_match_translate(s, can2man_table)\n",
    "    # print(man_phrases)\n",
    "    for i, phrases in enumerate(man_phrases):\n",
    "        if len(phrases) == 1:\n",
    "            continue\n",
    "        else:\n",
    "            best_ppl = float(\"+inf\")\n",
    "            best_phrase = \"\"\n",
    "            j = i + 1\n",
    "            while j < len(man_phrases) and man_phrases[j] == 1:\n",
    "                j += 1\n",
    "            backward_context = \"\".join(flatten(man_phrases[:i]))\n",
    "            forward_context = \"\".join(flatten(man_phrases[i + 1:j]))\n",
    "            # forward context is too small\n",
    "            while len(forward_context) < 5 and j < len(man_phrases):\n",
    "                forward_context += man_phrases[j][0]\n",
    "                j += 1\n",
    "            forward_context = chop_off_at_canto_char(chop_off_at_punctuation(forward_context))\n",
    "            # print(f\"i={i} backward_context={backward_context}, forward_context={forward_context}\")\n",
    "            for phrase in man_phrases[i]:\n",
    "                s = backward_context + phrase + forward_context\n",
    "                ppl = score_sentence_ppl(s)\n",
    "                # print(s, ppl)\n",
    "                if ppl < best_ppl:\n",
    "                    best_ppl = ppl\n",
    "                    best_phrase = phrase\n",
    "            man_phrases[i] = [best_phrase]\n",
    "    # print(man_phrases)\n",
    "    return \"\".join(flatten(man_phrases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'不好意思你小聲點，我在這裡正在做東西。'"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "can2man(\"唔該你細聲啲，我喺度做緊嘢。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'哪一個調整到一本書散了架子'"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "can2man(\"邊個整到本書甩皮甩骨\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'他舉重嗰時掬先測量人的精神狀態堅持住，終於破了世界紀錄'"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "can2man(\"佢舉重嗰時掬住度氣堅持住，終於破咗世界紀錄\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'現而今男子100公尺的世界記錄是9.58秒。'"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "can2man(\"而家男子100米嘅世界記錄係9.58秒。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dev.can\", \"r\") as input_file, open(\"dev.pred.man\", \"w+\") as output_file:\n",
    "    for line in input_file.read().splitlines()[0:100]:\n",
    "        output_file.write(can2man(line) + \"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

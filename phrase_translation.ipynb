{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Cantonese to Mandarin phrase table of size 11753\n",
      "[('少少', ['一丁點兒', '一點兒', '一點', '一點點兒', '很少份量', '很少']), ('一上一落', ['一上一下']), ('下', ['一下']), ('一搊', ['一串']), ('啲', ['一些', '些', '某些', '這些']), ('單嘢', ['一件事']), ('件', ['一件']), ('一班', ['一伙', '全班', '那班']), ('單拖', ['一個人']), ('獨贏', ['一個人得頭彩'])]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "can2man_table = defaultdict(list)\n",
    "\n",
    "with open(\"phrase_table.txt\", \"r\") as input_file:\n",
    "    for line in input_file.read().splitlines():\n",
    "        [man_word, can_word] = line.split(\"|\")\n",
    "        can2man_table[can_word].append(man_word)\n",
    "\n",
    "print(f\"Generated Cantonese to Mandarin phrase table of size {len(can2man_table)}\")\n",
    "print(list(can2man_table.items())[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A sample of common traditional characters:  ['溪', '建', '蠱', '鉑', '誼', '哨', '頰', '勢', '汐', '園']\n"
     ]
    }
   ],
   "source": [
    "common_trad_chars = None\n",
    "\n",
    "with open(\"common_trad_chars.txt\", \"r\") as input_file:\n",
    "    common_trad_chars = set(input_file.read())\n",
    "\n",
    "print(\"A sample of common traditional characters: \", list(common_trad_chars)[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/kk/n4ff6h1n3t170b1m4zv09yf40000gn/T/jieba.cache\n",
      "Loading model cost 0.358 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 526508 Mandarin words\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from StarCC import PresetConversion\n",
    "convert = PresetConversion(src='cn', dst='hk', with_phrase=False)\n",
    "\n",
    "# df = pd.read_csv(\"common_man_words.csv\", sep=\"\\t\")\n",
    "# common_man_words = { convert(word) for word in df[\"word\"] }\n",
    "common_man_words = set()\n",
    "with open(\"common_man_words.dict.yaml\", \"r\") as input_file:\n",
    "    for line in input_file.read().splitlines():\n",
    "        if not line.startswith(\"#\"):\n",
    "            word = line.split(\"\\t\")[0]\n",
    "            common_man_words.add(convert(word))\n",
    "\n",
    "with open(\"common_man_words.txt\", \"w+\") as output_file:\n",
    "    for word in common_man_words:\n",
    "        output_file.write(word + \"\\n\")\n",
    "\n",
    "print(f\"Got {len(common_man_words)} Mandarin words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 92568 Cantonese words\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"common_can_words.csv\", sep=\",\")\n",
    "common_can_words = set(df[\"char\"])\n",
    "\n",
    "print(f\"Got {len(common_can_words)} Cantonese words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 48844 common words\n",
      "Added 48820 shared words to can2man_table\n"
     ]
    }
   ],
   "source": [
    "common_words = common_can_words.intersection(common_man_words)\n",
    "\n",
    "num_added_words = 0\n",
    "for word in common_words:\n",
    "    if not word in can2man_table or not word in can2man_table[word]:\n",
    "        num_added_words += 1\n",
    "        can2man_table[word].append(word)\n",
    "\n",
    "with open(\"can2man_phrase_table.txt\", \"w+\") as output_file:\n",
    "    for can, mans in can2man_table.items():\n",
    "        for man in mans:\n",
    "            output_file.write(can + \"|\" + man + \"\\n\")\n",
    "\n",
    "print(f\"Got {len(common_words)} common words\")\n",
    "print(f\"Added {num_added_words} shared words to can2man_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def longest_match_translate(s, phrase_table):\n",
    "    man_phrases: list[list[str]] = []\n",
    "    oov_word = \"\"\n",
    "    while s:\n",
    "        longest_match = None\n",
    "        for phrase in phrase_table:\n",
    "            if s.startswith(phrase) and (longest_match is None or len(phrase) > len(longest_match)):\n",
    "                longest_match = phrase\n",
    "        if longest_match:\n",
    "            if len(oov_word) > 0:\n",
    "                man_phrases.append([oov_word])\n",
    "                oov_word = \"\"\n",
    "            can_original = [longest_match] if len(longest_match) <= 1 and all(c in common_trad_chars for c in longest_match) else []\n",
    "            man_phrase = phrase_table[longest_match]\n",
    "            man_phrases.append(can_original + man_phrase)\n",
    "            s = s[len(longest_match):].lstrip()\n",
    "        else:\n",
    "            oov_word += s[0]\n",
    "            s = s[1:].lstrip()\n",
    "    if len(oov_word) > 0:\n",
    "        man_phrases.append([oov_word])\n",
    "    # Merge anchor phrases (those with a single mandarin translation)\n",
    "    i = 0\n",
    "    merged_man_phrases = []\n",
    "    while i < len(man_phrases):\n",
    "        merged_phrase = \"\"\n",
    "        while i < len(man_phrases) and len(man_phrases[i]) == 1:\n",
    "            merged_phrase += man_phrases[i][0]\n",
    "            i += 1\n",
    "        if len(merged_phrase) > 0:\n",
    "            merged_man_phrases.append([merged_phrase])\n",
    "            merged_phrase = \"\"\n",
    "        else:\n",
    "            merged_man_phrases.append(man_phrases[i])\n",
    "            i += 1\n",
    "    return merged_man_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['不好意思', '勞', '勞駕', '有勞您', '請你', '請', '請教', '謝', '謝謝', '麻煩您'],\n",
       " ['你小聲點，我'],\n",
       " ['在此', '在這裡', '在這邊', '在那裡', '在那邊'],\n",
       " ['正在做'],\n",
       " ['事情', '東西'],\n",
       " ['。']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "longest_match_translate(\"唔該你細聲啲，我喺度做緊嘢。\", can2man_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast, GPT2LMHeadModel\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese')\n",
    "model = GPT2LMHeadModel.from_pretrained('ckiplab/gpt2-base-chinese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import torch\n",
    "\n",
    "def score_sentence_ppl(s: str) -> float:\n",
    "    test = Dataset.from_dict({\n",
    "        \"text\": [s],\n",
    "    })\n",
    "    encodings = tokenizer(\"\\n\\n\".join(test[\"text\"]), return_tensors=\"pt\")\n",
    "\n",
    "    max_length = model.config.n_positions\n",
    "    stride = 512\n",
    "    seq_len = encodings.input_ids.size(1)\n",
    "\n",
    "    nlls = []\n",
    "    prev_end_loc = 0\n",
    "    for begin_loc in range(0, seq_len, stride):\n",
    "        end_loc = min(begin_loc + max_length, seq_len)\n",
    "        trg_len = end_loc - prev_end_loc  # may be different from stride on last loop\n",
    "        input_ids = encodings.input_ids[:, begin_loc:end_loc]\n",
    "        target_ids = input_ids.clone()\n",
    "        target_ids[:, :-trg_len] = -100\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, labels=target_ids)\n",
    "\n",
    "            # loss is calculated using CrossEntropyLoss which averages over input tokens.\n",
    "            # Multiply it with trg_len to get the summation instead of average.\n",
    "            # We will take average over all the tokens to get the true average\n",
    "            # in the last step of this example.\n",
    "            neg_log_likelihood = outputs.loss * trg_len\n",
    "\n",
    "        nlls.append(neg_log_likelihood)\n",
    "\n",
    "        prev_end_loc = end_loc\n",
    "        if end_loc == seq_len:\n",
    "            break\n",
    "\n",
    "    ppl = torch.exp(torch.stack(nlls).sum() / end_loc).item()\n",
    "    return ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex\n",
    "\n",
    "# match any unicode punctuation character and anything after it\n",
    "punctuation_pattern = regex.compile(r\"\\p{P}+.*\", flags=regex.UNICODE)\n",
    "chinese_char_pattern = regex.compile(r\"[\\u4e00-\\u9fff]\")\n",
    "\n",
    "def chop_off_at_punctuation(s: str) -> str:\n",
    "    match = punctuation_pattern.search(s)\n",
    "    if match:\n",
    "        index = match.start()\n",
    "        return s[:index]\n",
    "    else:\n",
    "        return s\n",
    "\n",
    "def chop_off_at_canto_char(s: str) -> str:\n",
    "    for i, c in enumerate(s):\n",
    "        if chinese_char_pattern.match(c) and not c in common_trad_chars:\n",
    "            return s[:i]\n",
    "    return s\n",
    "\n",
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]\n",
    "\n",
    "def can2man(s: str) -> str:\n",
    "    man_phrases = longest_match_translate(s, can2man_table)\n",
    "    # print(man_phrases)\n",
    "    for i, phrases in enumerate(man_phrases):\n",
    "        if len(phrases) == 1:\n",
    "            continue\n",
    "        else:\n",
    "            best_ppl = float(\"+inf\")\n",
    "            best_phrase = \"\"\n",
    "            j = i + 1\n",
    "            while j < len(man_phrases) and man_phrases[j] == 1:\n",
    "                j += 1\n",
    "            backward_context = \"\".join(flatten(man_phrases[:i]))\n",
    "            forward_context = \"\".join(flatten(man_phrases[i + 1:j]))\n",
    "            # forward context is too small\n",
    "            while len(forward_context) < 10 and j < len(man_phrases):\n",
    "                forward_context += man_phrases[j][0]\n",
    "                j += 1\n",
    "            forward_context = chop_off_at_canto_char(chop_off_at_punctuation(forward_context))\n",
    "            # print(f\"i={i} backward_context={backward_context}, forward_context={forward_context}\")\n",
    "            for phrase in man_phrases[i]:\n",
    "                s = backward_context + phrase + forward_context\n",
    "                ppl = score_sentence_ppl(s)\n",
    "                # print(s, ppl)\n",
    "                if ppl < best_ppl:\n",
    "                    best_ppl = ppl\n",
    "                    best_phrase = phrase\n",
    "            man_phrases[i] = [best_phrase]\n",
    "    # print(man_phrases)\n",
    "    return \"\".join(flatten(man_phrases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'不好意思你小聲點，我在這裡正在做東西。'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "can2man(\"唔該你細聲啲，我喺度做緊嘢。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'哪一個調整到一本書散了架子'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "can2man(\"邊個整到本書甩皮甩骨\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'他舉重嗰時掬先測量人的精神狀態堅持住，終於破了世界紀錄'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "can2man(\"佢舉重嗰時掬住度氣堅持住，終於破咗世界紀錄\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'現下男子100公尺的世界記錄是9.58秒。'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "can2man(\"而家男子100米嘅世界記錄係9.58秒。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [06:09<00:00,  2.71it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "with open(\"dev.can\", \"r\") as input_file, open(\"dev.pred.base.man\", \"w+\") as output_file:\n",
    "    for line in tqdm(input_file.read().splitlines()[0:1000]):\n",
    "        output_file.write(can2man(line) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing first few pairs added from wordshk\n",
      "Adding the pair 賀 -> ['慶祝', '祝賀']\n",
      "Adding the pair 田螺厴 -> ['鰓蓋']\n",
      "Adding the pair 腍滋滋 -> ['柔軟的']\n",
      "Adding the pair 淋滋滋 -> ['柔軟的']\n",
      "Adding the pair 走犯 -> ['逃犯']\n",
      "Adding the pair 狗 -> ['狡猾', '狡猾', '狡猾', '棘手的', '不配', '性格低下', '狗', '狗']\n",
      "Adding the pair 打正旗號 -> ['公開地']\n",
      "Added 14272 new words from wordshk to can2man_table\n"
     ]
    }
   ],
   "source": [
    "# Extend phrase table with wordshk\n",
    "import json\n",
    "import math\n",
    "\n",
    "with open(\"wordshk_phrase_table.json\", \"r\") as input_file:\n",
    "    wordshk_table = json.load(input_file)\n",
    "\n",
    "def max_man_len(can_word_len: int) -> int:\n",
    "    return math.ceil(-2 * math.tanh(.5 * can_word_len - 1.9) + 3.1)\n",
    "\n",
    "print(f\"Showing first few pairs added from wordshk\")\n",
    "num_added_words = 0\n",
    "for word, mans in wordshk_table.items():\n",
    "    if not word in can2man_table or not word in can2man_table[word]:\n",
    "        num_added_words += 1\n",
    "        mans = [m for ms in mans for m in ms if len(m) <= max_man_len(len(word))]\n",
    "        if num_added_words <= 10 and len(mans) > 0:\n",
    "            print(f\"Adding the pair {word} -> {mans}\")\n",
    "        if len(mans) > 0:\n",
    "            can2man_table[word].extend(mans)\n",
    "\n",
    "print(f\"Added {num_added_words} new words from wordshk to can2man_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'不好意思你小聲點，我在這裡正在做東西。'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "can2man(\"唔該你細聲啲，我喺度做緊嘢。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'哪一個受傷一本書散了架子'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "can2man(\"邊個整到本書甩皮甩骨\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'他舉重什麼時候提升住程度人的精神狀態堅持住，終於打破了世界紀錄'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "can2man(\"佢舉重嗰時掬住度氣堅持住，終於破咗世界紀錄\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'現下男子100公尺的世界記錄是9.58秒。'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "can2man(\"而家男子100米嘅世界記錄係9.58秒。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [10:05<00:00,  1.65it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "with open(\"dev.can\", \"r\") as input_file, open(\"dev.pred.wordshk.man\", \"w+\") as output_file:\n",
    "    for line in tqdm(input_file.read().splitlines()[0:1000]):\n",
    "        output_file.write(can2man(line) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identity charBLEU: 11.916798739593405\n",
      "Identity CHRF: 11.892985577296292\n",
      "\n",
      "Phrase-Base charBLEU: 18.834564873844943\n",
      "Phrase-Base CHRF: 20.091559992795194\n",
      "\n",
      "Phrase-Wordshk charBLEU: 12.415971775936248\n",
      "Phrase-Wordshk CHRF: 16.061245936906026\n"
     ]
    }
   ],
   "source": [
    "# Measure BLEU of base model\n",
    "\n",
    "import os\n",
    "import sacrebleu\n",
    "\n",
    "def eval_bleu(ref, hyp):\n",
    "    \"\"\"\n",
    "    Given a file of hypothesis and reference files,\n",
    "    evaluate the BLEU score using Moses scripts.\n",
    "    \"\"\"\n",
    "    assert os.path.isfile(ref) and os.path.isfile(hyp)\n",
    "    with open(ref, \"r\") as ref_file, open(hyp, \"r\") as hyp_file:\n",
    "        refs = [ref_file.read().splitlines()]\n",
    "        hyp = hyp_file.read().splitlines()\n",
    "        bleu = sacrebleu.BLEU(trg_lang=\"zh\")\n",
    "        return bleu.corpus_score(hyp, refs).score\n",
    "\n",
    "\n",
    "def eval_chrf(ref, hyp):\n",
    "    \"\"\"\n",
    "    Given a file of hypothesis and reference files,\n",
    "    evaluate the BLEU score using Moses scripts.\n",
    "    \"\"\"\n",
    "    assert os.path.isfile(ref) and os.path.isfile(hyp)\n",
    "    with open(ref, \"r\") as ref_file, open(hyp, \"r\") as hyp_file:\n",
    "        refs = [ref_file.read().splitlines()]\n",
    "        hyp = hyp_file.read().splitlines()\n",
    "        chrf = sacrebleu.CHRF()\n",
    "        return chrf.corpus_score(hyp, refs).score\n",
    "\n",
    "\n",
    "print(\"Identity charBLEU:\", eval_bleu(\"dev.man\", \"dev.can\"))\n",
    "print(\"Identity CHRF:\", eval_chrf(\"dev.man\", \"dev.can\"))\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Phrase-Base charBLEU:\", eval_bleu(\"dev.man\", \"dev.pred.base.man\"))\n",
    "print(\"Phrase-Base CHRF:\", eval_chrf(\"dev.man\", \"dev.pred.base.man\"))\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Phrase-Wordshk charBLEU:\", eval_bleu(\"dev.man\", \"dev.pred.wordshk.man\"))\n",
    "print(\"Phrase-Wordshk CHRF:\", eval_chrf(\"dev.man\", \"dev.pred.wordshk.man\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Cantonese to Mandarin phrase table of size 11753\n",
      "[('少少', ['一丁點兒', '一點兒', '一點', '一點點兒', '很少份量', '很少']), ('一上一落', ['一上一下']), ('下', ['一下']), ('一搊', ['一串']), ('啲', ['一些', '些', '某些', '這些']), ('單嘢', ['一件事']), ('件', ['一件']), ('一班', ['一伙', '全班', '那班']), ('單拖', ['一個人']), ('獨贏', ['一個人得頭彩'])]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "can2man_table = defaultdict(list)\n",
    "\n",
    "with open(\"phrase_table.txt\", \"r\") as input_file:\n",
    "    for line in input_file.read().splitlines():\n",
    "        [man_word, can_word] = line.split(\"|\")\n",
    "        can2man_table[can_word].append(man_word)\n",
    "\n",
    "print(f\"Generated Cantonese to Mandarin phrase table of size {len(can2man_table)}\")\n",
    "print(list(can2man_table.items())[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A sample of common traditional characters:  ['蠍', '元', '癰', '額', '攀', '法', '爐', '戊', '莆', '穢']\n"
     ]
    }
   ],
   "source": [
    "common_trad_chars = None\n",
    "\n",
    "with open(\"common_trad_chars.txt\", \"r\") as input_file:\n",
    "    common_trad_chars = set(input_file.read())\n",
    "\n",
    "print(\"A sample of common traditional characters: \", list(common_trad_chars)[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 55735 Mandarin words\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from StarCC import PresetConversion\n",
    "convert = PresetConversion(src='cn', dst='hk', with_phrase=False)\n",
    "\n",
    "df = pd.read_csv(\"common_man_words.csv\", sep=\"\\t\")\n",
    "common_man_words = { convert(word) for word in df[\"word\"] }\n",
    "\n",
    "print(f\"Got {len(common_man_words)} Mandarin words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 92568 Cantonese words\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"common_can_words.csv\", sep=\",\")\n",
    "common_can_words = set(df[\"char\"])\n",
    "\n",
    "print(f\"Got {len(common_can_words)} Cantonese words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 28543 common words\n",
      "Added 28523 shared words to can2man_table\n"
     ]
    }
   ],
   "source": [
    "common_words = common_can_words.intersection(common_man_words)\n",
    "\n",
    "num_added_words = 0\n",
    "for word in common_words:\n",
    "    if not word in can2man_table or not word in can2man_table[word]:\n",
    "        num_added_words += 1\n",
    "        can2man_table[word].append(word)\n",
    "\n",
    "print(f\"Got {len(common_words)} common words\")\n",
    "print(f\"Added {num_added_words} shared words to can2man_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def longest_match_translate(s, phrase_table):\n",
    "    man_phrases: list[list[str]] = []\n",
    "    oov_word = \"\"\n",
    "    while s:\n",
    "        longest_match = None\n",
    "        for phrase in phrase_table:\n",
    "            if s.startswith(phrase) and (longest_match is None or len(phrase) > len(longest_match)):\n",
    "                longest_match = phrase\n",
    "        if longest_match:\n",
    "            if len(oov_word) > 0:\n",
    "                man_phrases.append([oov_word])\n",
    "                oov_word = \"\"\n",
    "            can_original = [longest_match] if len(longest_match) <= 1 and all(c in common_trad_chars for c in longest_match) else []\n",
    "            man_phrase = phrase_table[longest_match]\n",
    "            man_phrases.append(can_original + man_phrase)\n",
    "            s = s[len(longest_match):].lstrip()\n",
    "        else:\n",
    "            oov_word += s[0]\n",
    "            s = s[1:].lstrip()\n",
    "    if len(oov_word) > 0:\n",
    "        man_phrases.append([oov_word])\n",
    "    # Merge anchor phrases (those with a single mandarin translation)\n",
    "    i = 0\n",
    "    merged_man_phrases = []\n",
    "    while i < len(man_phrases):\n",
    "        merged_phrase = \"\"\n",
    "        while i < len(man_phrases) and len(man_phrases[i]) == 1:\n",
    "            merged_phrase += man_phrases[i][0]\n",
    "            i += 1\n",
    "        if len(merged_phrase) > 0:\n",
    "            merged_man_phrases.append([merged_phrase])\n",
    "            merged_phrase = \"\"\n",
    "        else:\n",
    "            merged_man_phrases.append(man_phrases[i])\n",
    "            i += 1\n",
    "    return merged_man_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['不好意思', '勞', '勞駕', '有勞您', '請你', '請', '請教', '謝', '謝謝', '麻煩您'],\n",
       " ['你小聲點，我'],\n",
       " ['在此', '在這裡', '在這邊', '在那裡', '在那邊'],\n",
       " ['正在做'],\n",
       " ['事情', '東西'],\n",
       " ['。']]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "longest_match_translate(\"唔該你細聲啲，我喺度做緊嘢。\", can2man_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast, GPT2LMHeadModel\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese')\n",
    "model = GPT2LMHeadModel.from_pretrained('ckiplab/gpt2-base-chinese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import torch\n",
    "\n",
    "def score_sentence_ppl(s: str) -> float:\n",
    "    test = Dataset.from_dict({\n",
    "        \"text\": [s],\n",
    "    })\n",
    "    encodings = tokenizer(\"\\n\\n\".join(test[\"text\"]), return_tensors=\"pt\")\n",
    "\n",
    "    max_length = model.config.n_positions\n",
    "    stride = 512\n",
    "    seq_len = encodings.input_ids.size(1)\n",
    "\n",
    "    nlls = []\n",
    "    prev_end_loc = 0\n",
    "    for begin_loc in range(0, seq_len, stride):\n",
    "        end_loc = min(begin_loc + max_length, seq_len)\n",
    "        trg_len = end_loc - prev_end_loc  # may be different from stride on last loop\n",
    "        input_ids = encodings.input_ids[:, begin_loc:end_loc]\n",
    "        target_ids = input_ids.clone()\n",
    "        target_ids[:, :-trg_len] = -100\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, labels=target_ids)\n",
    "\n",
    "            # loss is calculated using CrossEntropyLoss which averages over input tokens.\n",
    "            # Multiply it with trg_len to get the summation instead of average.\n",
    "            # We will take average over all the tokens to get the true average\n",
    "            # in the last step of this example.\n",
    "            neg_log_likelihood = outputs.loss * trg_len\n",
    "\n",
    "        nlls.append(neg_log_likelihood)\n",
    "\n",
    "        prev_end_loc = end_loc\n",
    "        if end_loc == seq_len:\n",
    "            break\n",
    "\n",
    "    ppl = torch.exp(torch.stack(nlls).sum() / end_loc).item()\n",
    "    return ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex\n",
    "\n",
    "# match any unicode punctuation character and anything after it\n",
    "punctuation_pattern = regex.compile(r\"\\p{P}+.*\", flags=regex.UNICODE)\n",
    "chinese_char_pattern = regex.compile(r\"[\\u4e00-\\u9fff]\")\n",
    "\n",
    "def chop_off_at_punctuation(s: str) -> str:\n",
    "    match = punctuation_pattern.search(s)\n",
    "    if match:\n",
    "        index = match.start()\n",
    "        return s[:index]\n",
    "    else:\n",
    "        return s\n",
    "\n",
    "def chop_off_at_canto_char(s: str) -> str:\n",
    "    for i, c in enumerate(s):\n",
    "        if chinese_char_pattern.match(c) and not c in common_trad_chars:\n",
    "            return s[:i]\n",
    "    return s\n",
    "\n",
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]\n",
    "\n",
    "def can2man(s: str) -> str:\n",
    "    man_phrases = longest_match_translate(s, can2man_table)\n",
    "    print(man_phrases)\n",
    "    for i, phrases in enumerate(man_phrases):\n",
    "        if len(phrases) == 1:\n",
    "            continue\n",
    "        else:\n",
    "            best_ppl = float(\"+inf\")\n",
    "            best_phrase = \"\"\n",
    "            j = i + 1\n",
    "            while j < len(man_phrases) and man_phrases[j] == 1:\n",
    "                j += 1\n",
    "            backward_context = \"\".join(flatten(man_phrases[:i]))\n",
    "            forward_context = \"\".join(flatten(man_phrases[i + 1:j]))\n",
    "            # forward context is too small\n",
    "            while len(forward_context) < 5 and j < len(man_phrases):\n",
    "                forward_context += man_phrases[j][0]\n",
    "                j += 1\n",
    "            forward_context = chop_off_at_canto_char(chop_off_at_punctuation(forward_context))\n",
    "            print(f\"i={i} backward_context={backward_context}, forward_context={forward_context}\")\n",
    "            for phrase in man_phrases[i]:\n",
    "                s = backward_context + phrase + forward_context\n",
    "                ppl = score_sentence_ppl(s)\n",
    "                print(s, ppl)\n",
    "                if ppl < best_ppl:\n",
    "                    best_ppl = ppl\n",
    "                    best_phrase = phrase\n",
    "            man_phrases[i] = [best_phrase]\n",
    "    print(man_phrases)\n",
    "    return \"\".join(flatten(man_phrases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['不好意思', '勞', '勞駕', '有勞您', '請你', '請', '請教', '謝', '謝謝', '麻煩您'], ['你小聲點，我'], ['在此', '在這裡', '在這邊', '在那裡', '在那邊'], ['正在做'], ['事情', '東西'], ['。']]\n",
      "i=0 backward_context=, forward_context=你小聲點\n",
      "不好意思你小聲點 1170.188232421875\n",
      "勞你小聲點 23191.947265625\n",
      "勞駕你小聲點 23770.203125\n",
      "有勞您你小聲點 12233.4599609375\n",
      "請你你小聲點 5392.3173828125\n",
      "請你小聲點 6540.541015625\n",
      "請教你小聲點 3959.528564453125\n",
      "謝你小聲點 8292.0556640625\n",
      "謝謝你小聲點 2870.170654296875\n",
      "麻煩您你小聲點 7786.3974609375\n",
      "i=2 backward_context=不好意思你小聲點，我, forward_context=正在做事情\n",
      "不好意思你小聲點，我在此正在做事情 290.8885803222656\n",
      "不好意思你小聲點，我在這裡正在做事情 186.99769592285156\n",
      "不好意思你小聲點，我在這邊正在做事情 234.96160888671875\n",
      "不好意思你小聲點，我在那裡正在做事情 191.083740234375\n",
      "不好意思你小聲點，我在那邊正在做事情 232.5902557373047\n",
      "i=4 backward_context=不好意思你小聲點，我在這裡正在做, forward_context=\n",
      "不好意思你小聲點，我在這裡正在做事情 186.99769592285156\n",
      "不好意思你小聲點，我在這裡正在做東西 173.3293914794922\n",
      "[['不好意思'], ['你小聲點，我'], ['在這裡'], ['正在做'], ['東西'], ['。']]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'不好意思你小聲點，我在這裡正在做東西。'"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "can2man(\"唔該你細聲啲，我喺度做緊嘢。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['哪一個', '哪位'], ['整', '修理', '整理', '生產', '製作', '製造', '調整', '造'], ['到', '達到'], ['一本書', '書本'], ['散了架子', '皮開骨散']]\n",
      "i=0 backward_context=, forward_context=整到一本書\n",
      "哪一個整到一本書 1213.12451171875\n",
      "哪位整到一本書 3353.59228515625\n",
      "i=1 backward_context=哪一個, forward_context=到一本書散了架子\n",
      "哪一個整到一本書散了架子 1153.7410888671875\n",
      "哪一個修理到一本書散了架子 826.3753662109375\n",
      "哪一個整理到一本書散了架子 742.5451049804688\n",
      "哪一個生產到一本書散了架子 768.58544921875\n",
      "哪一個製作到一本書散了架子 782.8555297851562\n",
      "哪一個製造到一本書散了架子 917.5545654296875\n",
      "哪一個調整到一本書散了架子 655.1337890625\n",
      "哪一個造到一本書散了架子 1464.9222412109375\n",
      "i=2 backward_context=哪一個調整, forward_context=一本書散了架子\n",
      "哪一個調整到一本書散了架子 655.1337890625\n",
      "哪一個調整達到一本書散了架子 793.9451293945312\n",
      "i=3 backward_context=哪一個調整到, forward_context=散了架子\n",
      "哪一個調整到一本書散了架子 655.1337890625\n",
      "哪一個調整到書本散了架子 1099.811279296875\n",
      "i=4 backward_context=哪一個調整到一本書, forward_context=\n",
      "哪一個調整到一本書散了架子 655.1337890625\n",
      "哪一個調整到一本書皮開骨散 1256.7835693359375\n",
      "[['哪一個'], ['調整'], ['到'], ['一本書'], ['散了架子']]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'哪一個調整到一本書散了架子'"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "can2man(\"邊個整到本書甩皮甩骨\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['他', '她', '它'], ['舉重嗰時掬'], ['住', '先'], ['度', '反復衡量', '度數', '揣度', '測量', '琢磨', '程度', '處', '裡', '那裏', '量'], ['氣', '人的精神狀態', '心情', '氣息', '氣體', '病象', '空氣'], ['堅持'], ['住', '先'], ['，終於破了'], ['生活', '世界'], ['紀錄']]\n",
      "i=0 backward_context=, forward_context=舉重\n",
      "他舉重 10582.169921875\n",
      "她舉重 16104.935546875\n",
      "它舉重 26628.703125\n",
      "i=2 backward_context=他舉重嗰時掬, forward_context=度氣堅持住\n",
      "他舉重嗰時掬住度氣堅持住 12438.759765625\n",
      "他舉重嗰時掬先度氣堅持住 11879.0673828125\n",
      "i=3 backward_context=他舉重嗰時掬先, forward_context=氣堅持住\n",
      "他舉重嗰時掬先度氣堅持住 11879.0673828125\n",
      "他舉重嗰時掬先反復衡量氣堅持住 10575.3505859375\n",
      "他舉重嗰時掬先度數氣堅持住 10932.5673828125\n",
      "他舉重嗰時掬先揣度氣堅持住 16721.763671875\n",
      "他舉重嗰時掬先測量氣堅持住 8536.3984375\n",
      "他舉重嗰時掬先琢磨氣堅持住 8938.1884765625\n",
      "他舉重嗰時掬先程度氣堅持住 10696.958984375\n",
      "他舉重嗰時掬先處氣堅持住 16819.1953125\n",
      "他舉重嗰時掬先裡氣堅持住 17449.294921875\n",
      "他舉重嗰時掬先那裏氣堅持住 12883.935546875\n",
      "他舉重嗰時掬先量氣堅持住 12750.7490234375\n",
      "i=4 backward_context=他舉重嗰時掬先測量, forward_context=堅持住\n",
      "他舉重嗰時掬先測量氣堅持住 8536.3984375\n",
      "他舉重嗰時掬先測量人的精神狀態堅持住 1234.3973388671875\n",
      "他舉重嗰時掬先測量心情堅持住 4336.623046875\n",
      "他舉重嗰時掬先測量氣息堅持住 6795.72509765625\n",
      "他舉重嗰時掬先測量氣體堅持住 4760.07861328125\n",
      "他舉重嗰時掬先測量病象堅持住 8784.0869140625\n",
      "他舉重嗰時掬先測量空氣堅持住 4716.76513671875\n",
      "i=6 backward_context=他舉重嗰時掬先測量人的精神狀態堅持, forward_context=\n",
      "他舉重嗰時掬先測量人的精神狀態堅持住 1234.3973388671875\n",
      "他舉重嗰時掬先測量人的精神狀態堅持先 1241.2156982421875\n",
      "i=8 backward_context=他舉重嗰時掬先測量人的精神狀態堅持住，終於破了, forward_context=紀錄\n",
      "他舉重嗰時掬先測量人的精神狀態堅持住，終於破了生活紀錄 408.9526672363281\n",
      "他舉重嗰時掬先測量人的精神狀態堅持住，終於破了世界紀錄 282.650146484375\n",
      "[['他'], ['舉重嗰時掬'], ['先'], ['測量'], ['人的精神狀態'], ['堅持'], ['住'], ['，終於破了'], ['世界'], ['紀錄']]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'他舉重嗰時掬先測量人的精神狀態堅持住，終於破了世界紀錄'"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "can2man(\"佢舉重嗰時掬住度氣堅持住，終於破咗世界紀錄\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['現下', '現而今', '這會兒'], ['男子100'], ['米', '公尺'], ['的'], ['生活', '世界'], ['記錄是9.58秒。']]\n",
      "i=0 backward_context=, forward_context=男子100\n",
      "現下男子100 6701.765625\n",
      "現而今男子100 5827.7001953125\n",
      "這會兒男子100 7303.15625\n",
      "i=2 backward_context=現而今男子100, forward_context=的生活\n",
      "現而今男子100米的生活 1117.426513671875\n",
      "現而今男子100公尺的生活 673.7601318359375\n",
      "i=4 backward_context=現而今男子100公尺的, forward_context=記錄是9\n",
      "現而今男子100公尺的生活記錄是9 373.15582275390625\n",
      "現而今男子100公尺的世界記錄是9 190.93446350097656\n",
      "[['現而今'], ['男子100'], ['公尺'], ['的'], ['世界'], ['記錄是9.58秒。']]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'現而今男子100公尺的世界記錄是9.58秒。'"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "can2man(\"而家男子100米嘅世界記錄係9.58秒。\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A sample of common traditional characters:  ['晉', '辱', '金', '斂', '訃', '嬌', '疏', '逼', '碟', '漬']\n"
     ]
    }
   ],
   "source": [
    "common_trad_chars = None\n",
    "\n",
    "with open(\"common_trad_chars.txt\", \"r\") as input_file:\n",
    "    common_trad_chars = set(input_file.read())\n",
    "\n",
    "print(\"A sample of common traditional characters: \", list(common_trad_chars)[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "can2man_table = defaultdict(list)\n",
    "\n",
    "with open(\"can2man_phrase_table.txt\", \"r\") as input_file:\n",
    "    for line in input_file.read().splitlines():\n",
    "        [can_word, man_word] = line.split(\"|\")\n",
    "        can2man_table[can_word].append(man_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5635 one-to-many mappings (10% out of 58511).\n",
      "Average number of Mandarin translations for one-to-many: 2.7531499556344277\n"
     ]
    }
   ],
   "source": [
    "num_one_to_many = 0\n",
    "avg_one_to_many = 0\n",
    "\n",
    "for can_word, man_words in can2man_table.items():\n",
    "    if len(man_words) > 1:\n",
    "        num_one_to_many += 1\n",
    "        avg_one_to_many += len(man_words)\n",
    "\n",
    "avg_one_to_many /= num_one_to_many\n",
    "\n",
    "print(\"Found {} one-to-many mappings ({:.0%} out of {}).\".format(num_one_to_many, num_one_to_many / len(can2man_table), len(can2man_table)))\n",
    "print(\"Average number of Mandarin translations for one-to-many: {}\".format(avg_one_to_many))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def longest_match_translate(s, phrase_table):\n",
    "    man_phrases: list[list[str]] = []\n",
    "    oov_word = \"\"\n",
    "    while s:\n",
    "        longest_match = None\n",
    "        for phrase in phrase_table:\n",
    "            if s.startswith(phrase) and (longest_match is None or len(phrase) > len(longest_match)):\n",
    "                longest_match = phrase\n",
    "        if longest_match:\n",
    "            if len(oov_word) > 0:\n",
    "                man_phrases.append([oov_word])\n",
    "                oov_word = \"\"\n",
    "            can_original = [longest_match] if len(longest_match) <= 1 and all(c in common_trad_chars for c in longest_match) else []\n",
    "            man_phrase = phrase_table[longest_match]\n",
    "            man_phrases.append(can_original + man_phrase)\n",
    "            s = s[len(longest_match):].lstrip()\n",
    "        else:\n",
    "            oov_word += s[0]\n",
    "            s = s[1:].lstrip()\n",
    "    if len(oov_word) > 0:\n",
    "        man_phrases.append([oov_word])\n",
    "    # Merge anchor phrases (those with a single mandarin translation)\n",
    "    i = 0\n",
    "    merged_man_phrases = []\n",
    "    while i < len(man_phrases):\n",
    "        merged_phrase = \"\"\n",
    "        while i < len(man_phrases) and len(man_phrases[i]) == 1:\n",
    "            merged_phrase += man_phrases[i][0]\n",
    "            i += 1\n",
    "        if len(merged_phrase) > 0:\n",
    "            merged_man_phrases.append([merged_phrase])\n",
    "            merged_phrase = \"\"\n",
    "        else:\n",
    "            merged_man_phrases.append(man_phrases[i])\n",
    "            i += 1\n",
    "    return merged_man_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['不好意思', '勞', '勞駕', '有勞您', '請你', '請', '請教', '謝', '謝謝', '麻煩您'],\n",
       " ['你小聲點，我'],\n",
       " ['在此', '在這裡', '在這邊', '在那裡', '在那邊'],\n",
       " ['正在做'],\n",
       " ['事情', '東西'],\n",
       " ['。']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "longest_match_translate(\"唔該你細聲啲，我喺度做緊嘢。\", can2man_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast, GPT2LMHeadModel\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese')\n",
    "model = GPT2LMHeadModel.from_pretrained('ckiplab/gpt2-base-chinese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# https://huggingface.co/docs/transformers/perplexity\n",
    "def get_most_fluent_sentence_index(candidates: list[str]) -> int:\n",
    "    encodings = [tokenizer(candidate, return_tensors=\"pt\") for candidate in candidates]\n",
    "    ppls = []\n",
    "    for encoding in encodings:\n",
    "        target_ids_list = []\n",
    "        seq_len = encoding.input_ids.size(1)\n",
    "        for end_loc in range(2, seq_len + 1, 2):\n",
    "            target_ids = encoding.input_ids[0].clone()\n",
    "            target_ids[end_loc:] = -100\n",
    "            target_ids_list.append(target_ids)\n",
    "        target_ids = torch.stack(target_ids_list)\n",
    "        input_ids = encoding.input_ids.expand(target_ids.shape)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, labels=target_ids)\n",
    "            ppl = outputs.loss.item() * seq_len\n",
    "            ppls.append(ppl)\n",
    "    return torch.argmin(torch.tensor(ppls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex\n",
    "\n",
    "# match any unicode punctuation character and anything after it\n",
    "punctuation_pattern = regex.compile(r\"\\p{P}+.*\", flags=regex.UNICODE)\n",
    "chinese_char_pattern = regex.compile(r\"[\\u4e00-\\u9fff]\")\n",
    "\n",
    "def chop_off_at_punctuation(s: str) -> str:\n",
    "    match = punctuation_pattern.search(s)\n",
    "    if match:\n",
    "        index = match.start()\n",
    "        return s[:index]\n",
    "    else:\n",
    "        return s\n",
    "\n",
    "def chop_off_at_canto_char(s: str) -> str:\n",
    "    for i, c in enumerate(s):\n",
    "        if chinese_char_pattern.match(c) and not c in common_trad_chars:\n",
    "            return s[:i]\n",
    "    return s\n",
    "\n",
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]\n",
    "\n",
    "def can2man(s: str) -> str:\n",
    "    man_phrases = longest_match_translate(s, can2man_table)\n",
    "    # print(man_phrases)\n",
    "    for i, phrases in enumerate(man_phrases):\n",
    "        if len(phrases) == 1:\n",
    "            continue\n",
    "        else:\n",
    "            j = i + 1\n",
    "            while j < len(man_phrases) and man_phrases[j] == 1:\n",
    "                j += 1\n",
    "            backward_context = \"\".join(flatten(man_phrases[max(0, i-5):i]))\n",
    "            forward_context = \"\".join(flatten(man_phrases[i + 1:j]))\n",
    "            # forward context is too small\n",
    "            while len(forward_context) < 10 and j < len(man_phrases):\n",
    "                forward_context += man_phrases[j][0]\n",
    "                j += 1\n",
    "            forward_context = chop_off_at_canto_char(chop_off_at_punctuation(forward_context))\n",
    "            # print(f\"i={i} backward_context={backward_context}, forward_context={forward_context}\")\n",
    "            candidates = [backward_context + phrase + forward_context for phrase in man_phrases[i]]\n",
    "            j = get_most_fluent_sentence_index(candidates)\n",
    "            man_phrases[i] = [man_phrases[i][j]]\n",
    "    # print(man_phrases)\n",
    "    return \"\".join(flatten(man_phrases))\n",
    "\n",
    "import random\n",
    "def can2man_random(s: str) -> str:\n",
    "    man_phrases = longest_match_translate(s, can2man_table)\n",
    "    # print(man_phrases)\n",
    "    for i, phrases in enumerate(man_phrases):\n",
    "        if len(phrases) == 1:\n",
    "            continue\n",
    "        else:\n",
    "            man_phrases[i] = [random.choice(man_phrases[i])]\n",
    "    # print(man_phrases)\n",
    "    return \"\".join(flatten(man_phrases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "請你小聲點，我在這裡正在做東西。\n",
      "謝你小聲點，我在那裡正在做東西。\n"
     ]
    }
   ],
   "source": [
    "print(can2man(\"唔該你細聲啲，我喺度做緊嘢。\"))\n",
    "print(can2man_random(\"唔該你細聲啲，我喺度做緊嘢。\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "哪一個調整到一本書散了架子\n",
      "哪位製作到一本書皮開骨散\n"
     ]
    }
   ],
   "source": [
    "print(can2man(\"邊個整到本書甩皮甩骨\"))\n",
    "print(can2man_random(\"邊個整到本書甩皮甩骨\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "他舉重嗰時掬先度氣堅持先，終於破了世界紀錄\n",
      "她舉重嗰時掬先裡氣息堅持住，終於破了生活紀錄\n"
     ]
    }
   ],
   "source": [
    "print(can2man(\"佢舉重嗰時掬住度氣堅持住，終於破咗世界紀錄\"))\n",
    "print(can2man_random(\"佢舉重嗰時掬住度氣堅持住，終於破咗世界紀錄\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "現下男子100米的世界記錄是9.58秒。\n",
      "現而今男子100公尺的世界記錄是9.58秒。\n"
     ]
    }
   ],
   "source": [
    "print(can2man(\"而家男子100米嘅世界記錄係9.58秒。\"))\n",
    "print(can2man_random(\"而家男子100米嘅世界記錄係9.58秒。\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "絕對不可以同等，母語（粵語）一定是第一，至於英文你覺得重要不然問題，但地位不可以超越母語，你有見過其他國家（除了新加坡）會把母語ge地位擺是外語之後？\n",
      "絕對不可以同等，母語（粵語）一定是第一，至於英文你覺得重要可不是問題，但是地位不可以超越母語，你有見比其他國家（算術上的除法了新加坡）將會把母語ge地位擺設是外語之後？\n"
     ]
    }
   ],
   "source": [
    "print(can2man(\"絕對 唔 可以 同等 ， 母語 （ 粵語 ） 一 定係 第一 ， 至於 英文 你 覺得 重要 唔係 問題 ， 但 地位 唔 可以 超越 母語 ， 你 有 見 過 其他 國家 （ 除咗 新加坡 ） 會 將 母語 ge 地位 擺 係 外語 之後 ？\".replace(\" \", \"\")))\n",
    "print(can2man_random(\"絕對 唔 可以 同等 ， 母語 （ 粵語 ） 一 定係 第一 ， 至於 英文 你 覺得 重要 唔係 問題 ， 但 地位 唔 可以 超越 母語 ， 你 有 見 過 其他 國家 （ 除咗 新加坡 ） 會 將 母語 ge 地位 擺 係 外語 之後 ？\".replace(\" \", \"\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:26<00:00, 37.52it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "with open(\"dev.can\", \"r\") as input_file, open(\"dev.pred.random.man\", \"w+\") as output_file:\n",
    "    for line in tqdm(input_file.read().splitlines()[0:1000]):\n",
    "        output_file.write(can2man_random(line) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80000/80000 [1:16:57<00:00, 17.32it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "with open(\"can.txt\", \"r\") as input_file, open(\"man_40K_to_120K.txt\", \"w+\") as output_file:\n",
    "    for line in tqdm(input_file.read().splitlines()[40000:40000*3]):\n",
    "        output_file.write(can2man_random(line) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'不好意思你小聲點，我在這裡正在做東西。'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "can2man(\"唔該你細聲啲，我喺度做緊嘢。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'哪一個受傷一本書散了架子'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "can2man(\"邊個整到本書甩皮甩骨\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'他舉重什麼時候提升住程度人的精神狀態堅持住，終於打破了世界紀錄'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "can2man(\"佢舉重嗰時掬住度氣堅持住，終於破咗世界紀錄\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'現下男子100公尺的世界記錄是9.58秒。'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "can2man(\"而家男子100米嘅世界記錄係9.58秒。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [11:57<00:00,  1.39it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "with open(\"dev.can\", \"r\") as input_file, open(\"dev.pred.full.man\", \"w+\") as output_file:\n",
    "    for line in tqdm(input_file.read().splitlines()[0:1000]):\n",
    "        output_file.write(can2man(line) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identity charBLEU: 11.916798739593405\n",
      "Identity CHRF: 11.892985577296292\n",
      "\n",
      "Phrase-Random charBLEU: 16.844144675728426\n",
      "Phrase-Random CHRF: 17.04834182555853\n",
      "\n",
      "Phrase-Base charBLEU: 18.834564873844943\n",
      "Phrase-Base CHRF: 20.091559992795194\n",
      "\n",
      "Phrase-Wordshk charBLEU: 12.415971775936248\n",
      "Phrase-Wordshk CHRF: 16.061245936906026\n",
      "\n",
      "Phrase-Full charBLEU: 24.23631480403355\n",
      "Phrase-Full CHRF: 21.32881366574855\n"
     ]
    }
   ],
   "source": [
    "# Measure BLEU of base model\n",
    "\n",
    "import os\n",
    "import sacrebleu\n",
    "\n",
    "def eval_bleu(ref, hyp):\n",
    "    \"\"\"\n",
    "    Given a file of hypothesis and reference files,\n",
    "    evaluate the BLEU score using Moses scripts.\n",
    "    \"\"\"\n",
    "    assert os.path.isfile(ref) and os.path.isfile(hyp)\n",
    "    with open(ref, \"r\") as ref_file, open(hyp, \"r\") as hyp_file:\n",
    "        refs = [ref_file.read().splitlines()]\n",
    "        hyp = hyp_file.read().splitlines()\n",
    "        bleu = sacrebleu.BLEU(trg_lang=\"zh\")\n",
    "        return bleu.corpus_score(hyp, refs).score\n",
    "\n",
    "\n",
    "def eval_chrf(ref, hyp):\n",
    "    \"\"\"\n",
    "    Given a file of hypothesis and reference files,\n",
    "    evaluate the BLEU score using Moses scripts.\n",
    "    \"\"\"\n",
    "    assert os.path.isfile(ref) and os.path.isfile(hyp)\n",
    "    with open(ref, \"r\") as ref_file, open(hyp, \"r\") as hyp_file:\n",
    "        refs = [ref_file.read().splitlines()]\n",
    "        hyp = hyp_file.read().splitlines()\n",
    "        chrf = sacrebleu.CHRF()\n",
    "        return chrf.corpus_score(hyp, refs).score\n",
    "\n",
    "\n",
    "print(\"Identity charBLEU:\", eval_bleu(\"dev.man\", \"dev.can\"))\n",
    "print(\"Identity CHRF:\", eval_chrf(\"dev.man\", \"dev.can\"))\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Phrase-Random charBLEU:\", eval_bleu(\"dev.man\", \"dev.pred.random.man\"))\n",
    "print(\"Phrase-Random CHRF:\", eval_chrf(\"dev.man\", \"dev.pred.random.man\"))\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Phrase-Base charBLEU:\", eval_bleu(\"dev.man\", \"dev.pred.base.man\"))\n",
    "print(\"Phrase-Base CHRF:\", eval_chrf(\"dev.man\", \"dev.pred.base.man\"))\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Phrase-Wordshk charBLEU:\", eval_bleu(\"dev.man\", \"dev.pred.wordshk.man\"))\n",
    "print(\"Phrase-Wordshk CHRF:\", eval_chrf(\"dev.man\", \"dev.pred.wordshk.man\"))\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Phrase-Full charBLEU:\", eval_bleu(\"dev.man\", \"dev.pred.full.man\"))\n",
    "print(\"Phrase-Full CHRF:\", eval_chrf(\"dev.man\", \"dev.pred.full.man\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from joblib import Parallel, delayed\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "\n",
    "with open(\"cantonese_18M_freq_10_up.txt\", \"r\") as input_file, open(\"cantonese_18M_freq_10_up.pred.base.txt\", \"w+\") as output_file:\n",
    "    # Parallel(prefer=\"threads\", n_jobs=-1, verbose=2)(delayed(translate)(line) for line in input_file.read().splitlines()[0:100000])\n",
    "    for line in input_file.read().splitlines()[0:100000]:\n",
    "        output_file.write(can2man(line.replace(\" \", \"\")) + \"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
